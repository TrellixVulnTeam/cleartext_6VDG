{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>ClearText</center></h1>\n",
    "<center>Benjamin Wallace</center>\n",
    "<center>(2020)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *English as  Second Language* Market\n",
    "\n",
    "According to [TESOL](https://www.tesol.org/) (Teachers of English to Speakers of Other Languages), there are over [1.5 billion](https://www.internationalteflacademy.com/blog/report-from-tesol-2-billion-english-learners-worldwide) English language learners worldwide. A huge amount of human labour is involved in educating these learners. However, many learners may not be able to afford lesson costs and must resort to other methods.\n",
    "\n",
    "There is a large market for assisted language learning applications. For instance, [Forbes](https://www.forbes.com/sites/susanadams/2019/07/16/game-of-tongues-how-duolingo-built-a-700-million-business-with-its-addictive-language-learning-app/#5f99b9d83463) reports that [Duolingo](https://www.duolingo.com/) was valued at \\$700 million in 2017 and that [Babbel](https://www.babbel.com/) has a revenue of \\$115 million.\n",
    "\n",
    "However, apps like these are generally limited to basic language skills that do not transfer to real world use. In order to retain users that would otherwise outgrow them, app developers are forced to design increasingly complex and challenging language games, which requires extensive work by multi-lingual language and education experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Assisted Reading for Language Learners\n",
    "\n",
    "Many language learners make use of subtitles in film and other media to assist them in the learning process. An English language learner, for instance, might turn on English subtitles while watching a movie in English. The additional visual input helps learners with their oral comprehension skills.\n",
    "\n",
    "Unfortunately, a similar solution for text media is missing. A learner who desires to regularly read reports from a certain English language news source in order to improve their reading comprehension skills might be frustrated at the difficulty they encounter and the crudeness of existing forms of assistance, such as dictionaries, which do not take *context* into account.\n",
    "\n",
    "ClearText attempts to solve this problem through the use of text simplification technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Technology: Text Simplification\n",
    "\n",
    "*Text simplification* is a natural language processing task that seeks to transform a source text into a \"simpler\" (easier to read or understand) target text while preserving the meaning or content of the text. Note that this is distinct from text *summarization*: A summary of a text need not be simpler (and may even be denser and more complex) and a simplification may not provide a useful summary (it may be just as long, if not longer, than the original text).\n",
    "\n",
    "Text simplification can be understood as a form of *monolingual* machine translation. Indeed, this is the approach typically taken by many academic researchers on this problem. Unfortunately, at the time of writing we are not aware of any *production-ready* text simplification systems. Implementing such a system is one of the main technical obstacles we must overcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The WikiLarge and WikiSmall Datasets\n",
    "\n",
    "Encouraged by the incredible successes of natural language processing in the several years, we treat the text simplification problem as a machine learning problem. Precisely, we treat it as a neural machine translation problem. There are several datasets available for neural machine translation, but we will begin our work with the [WikiLarge and WikiSmall](https://github.com/XingxingZhang/dress) corpuses. Both corpuses, for the most part, consist of aligned text pairs automatically produced (using various similarity metrics) from corresponding articles on Wikipedia and the [Simple English Wikipedia](https://simple.wikipedia.org/).\n",
    "\n",
    "In order to run the following cells, the compressed file containing these datasets should be extracted into `../data/raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki(dataset):\n",
    "    wiki_dir = os.path.join('../data/raw/data-simplification', dataset)\n",
    "    prefix = 'PWKP_108016.tag.80.aner.ori' if dataset == 'wikismall' else 'wiki.full.aner.ori'\n",
    "    \n",
    "    data = []\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        for loc in ['src', 'dst']:\n",
    "            file_name = '.'.join([prefix, split, loc])\n",
    "            file_path = os.path.join(wiki_dir, file_name)\n",
    "            stream = io.open(file_path)\n",
    "            lines = stream.read().split('\\n')\n",
    "            data.append(lines)\n",
    "\n",
    "    src_train, dst_train, src_valid, dst_valid, src_test, dst_test = data\n",
    "    train = pd.DataFrame(zip(src_train, dst_train), columns=['source', 'target'])\n",
    "    valid = pd.DataFrame(zip(src_valid, dst_valid), columns=['source', 'target'])\n",
    "    test = pd.DataFrame(zip(src_test, dst_test), columns=['source', 'target'])\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = load_wiki('wikismall')\n",
    "\n",
    "print(f'Training examples: {len(train)}')\n",
    "print(f'Validation examples: {len(valid)}')\n",
    "print(f'Test examples: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset appears to be quite noisy, containing some nonsense examples such as the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "print(train.loc[i, 'source'])\n",
    "print(train.loc[i, 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also contains some low-quality simplifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "print(train.loc[i, 'source'])\n",
    "print(train.loc[i, 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set, on the other hand, comes from the [Turk corpus](https://github.com/cocoxu/simplification/). It was produced by humans on Amazon's [Mechanical Turk](https://www.mturk.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>ClearText</center></h1>\n",
    "<center>Benjamin Wallace</center>\n",
    "<center>(2020)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *English as  Second Language* Market\n",
    "\n",
    "According to [TESOL](https://www.tesol.org/) (Teachers of English to Speakers of Other Languages), there are over [1.5 billion](https://www.internationalteflacademy.com/blog/report-from-tesol-2-billion-english-learners-worldwide) English language learners worldwide. A huge amount of human labour is involved in educating these learners. However, many learners may not be able to afford lesson costs and must resort to other methods.\n",
    "\n",
    "There is a large market for assisted language learning applications. For instance, [Forbes](https://www.forbes.com/sites/susanadams/2019/07/16/game-of-tongues-how-duolingo-built-a-700-million-business-with-its-addictive-language-learning-app/#5f99b9d83463) reports that [Duolingo](https://www.duolingo.com/) was valued at \\$700 million in 2017 and that [Babbel](https://www.babbel.com/) has a revenue of \\$115 million.\n",
    "\n",
    "However, apps like these are generally limited to basic language skills that do not transfer to real world use. In order to retain users that would otherwise outgrow them, app developers are forced to design increasingly complex and challenging language games, which requires extensive work by multi-lingual language and education experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Assisted Reading for Language Learners\n",
    "\n",
    "Many language learners make use of subtitles in film and other media to assist them in the learning process. An English language learner, for instance, might turn on English subtitles while watching a movie in English. The additional visual input helps learners with their oral comprehension skills.\n",
    "\n",
    "Unfortunately, a similar solution for text media is missing. A learner who desires to regularly read reports from a certain English language news source in order to improve their reading comprehension skills might be frustrated at the difficulty they encounter and the crudeness of existing forms of assistance, such as dictionaries, which do not take *context* into account.\n",
    "\n",
    "ClearText attempts to solve this problem through the use of text simplification technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Technology: Text Simplification\n",
    "\n",
    "*Text simplification* is a natural language processing task that seeks to transform a source text into a \"simpler\" (easier to read or understand) target text while preserving the meaning or content of the text. Note that this is distinct from text *summarization*: A summary of a text need not be simpler (and may even be denser and more complex) and a simplification may not provide a useful summary (it may be just as long, if not longer, than the original text).\n",
    "\n",
    "Text simplification can be understood as a form of *monolingual* machine translation. Indeed, this is the approach typically taken by many academic researchers on this problem. Unfortunately, at the time of writing we are not aware of any *production-ready* text simplification systems. Implementing such a system is one of the main technical obstacles we must overcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The WikiLarge and WikiSmall Datasets\n",
    "\n",
    "Encouraged by the incredible successes of natural language processing in the several years, we treat the text simplification problem as a machine learning problem. Precisely, we treat it as a neural machine translation problem. There are several datasets available for neural machine translation, but we will begin our work with the [WikiLarge and WikiSmall](https://github.com/XingxingZhang/dress) corpuses. Both corpuses, for the most part, consist of aligned text pairs automatically produced (using various similarity metrics) from corresponding articles on Wikipedia and the [Simple English Wikipedia](https://simple.wikipedia.org/).\n",
    "\n",
    "In order to run the following cells, the compressed file containing these datasets should be extracted into `../data/raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki(dataset):\n",
    "    wiki_dir = os.path.join('../data/raw/data-simplification', dataset)\n",
    "    prefix = 'PWKP_108016.tag.80.aner.ori' if dataset == 'wikismall' else 'wiki.full.aner.ori'\n",
    "    \n",
    "    data = []\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        for loc in ['src', 'dst']:\n",
    "            file_name = '.'.join([prefix, split, loc])\n",
    "            file_path = os.path.join(wiki_dir, file_name)\n",
    "            stream = io.open(file_path)\n",
    "            lines = stream.read().split('\\n')\n",
    "            data.append(lines)\n",
    "\n",
    "    src_train, dst_train, src_valid, dst_valid, src_test, dst_test = data\n",
    "    train = pd.DataFrame(zip(src_train, dst_train), columns=['source', 'target'])\n",
    "    valid = pd.DataFrame(zip(src_valid, dst_valid), columns=['source', 'target'])\n",
    "    test = pd.DataFrame(zip(src_test, dst_test), columns=['source', 'target'])\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 88838\n",
      "Validation examples: 206\n",
      "Test examples: 101\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = load_wiki('wikismall')\n",
    "\n",
    "print(f'Training examples: {len(train)}')\n",
    "print(f'Validation examples: {len(valid)}')\n",
    "print(f'Test examples: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Demographics</td>\n",
       "      <td>People</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pony Canyon Popfrenzy Popsicle Records Poptone...</td>\n",
       "      <td>Pony Canyon Popfrenzy Poptones Pork Recordings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In 1199 , during the period of its highest spl...</td>\n",
       "      <td>Volterra .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The mills contributed to the growth of Minneap...</td>\n",
       "      <td>Minneapolis .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apart from being a member of the Brazilian Aca...</td>\n",
       "      <td>Brazilian Academy of Sciences .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nathan Fillion as Malcolm `` Mal '' Reynolds :...</td>\n",
       "      <td>Malcolm `` Mal '' Reynolds .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HostGIS HostGIS Linux is a Slackware based dis...</td>\n",
       "      <td>GoblinX HostGIS NimbleX .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nawansher Urban is one of the 51 Union Council...</td>\n",
       "      <td>District Government Abbottabad .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Joan Mondale , wife of Walter Mondale .</td>\n",
       "      <td>Joan Mondale , Wife of Walter Mondale .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>45 South Dakota SD S. Dak. .</td>\n",
       "      <td>Rhode Island R.I. South Carolina S.C. South Da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0                                       Demographics   \n",
       "1  Pony Canyon Popfrenzy Popsicle Records Poptone...   \n",
       "2  In 1199 , during the period of its highest spl...   \n",
       "3  The mills contributed to the growth of Minneap...   \n",
       "4  Apart from being a member of the Brazilian Aca...   \n",
       "5  Nathan Fillion as Malcolm `` Mal '' Reynolds :...   \n",
       "6  HostGIS HostGIS Linux is a Slackware based dis...   \n",
       "7  Nawansher Urban is one of the 51 Union Council...   \n",
       "8            Joan Mondale , wife of Walter Mondale .   \n",
       "9                       45 South Dakota SD S. Dak. .   \n",
       "\n",
       "                                              target  \n",
       "0                                             People  \n",
       "1  Pony Canyon Popfrenzy Poptones Pork Recordings...  \n",
       "2                                         Volterra .  \n",
       "3                                      Minneapolis .  \n",
       "4                    Brazilian Academy of Sciences .  \n",
       "5                       Malcolm `` Mal '' Reynolds .  \n",
       "6                          GoblinX HostGIS NimbleX .  \n",
       "7                   District Government Abbottabad .  \n",
       "8            Joan Mondale , Wife of Walter Mondale .  \n",
       "9  Rhode Island R.I. South Carolina S.C. South Da...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The program was transmitted by Onda Cero Radio...</td>\n",
       "      <td>Onda Cero .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyonce Sued For Copyright Infringement Track ...</td>\n",
       "      <td>Track listing Title !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cast and characters Humans Shia LaBeouf stars ...</td>\n",
       "      <td>It stars Shia LaBeouf as Sam Witwicky .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A sequel , A Hat Full of Sky , was published i...</td>\n",
       "      <td>It was published in 2003 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>However , it is equally likely to decay at any...</td>\n",
       "      <td>However , it is radioactive .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  The program was transmitted by Onda Cero Radio...   \n",
       "1  Beyonce Sued For Copyright Infringement Track ...   \n",
       "2  Cast and characters Humans Shia LaBeouf stars ...   \n",
       "3  A sequel , A Hat Full of Sky , was published i...   \n",
       "4  However , it is equally likely to decay at any...   \n",
       "\n",
       "                                    target  \n",
       "0                              Onda Cero .  \n",
       "1                    Track listing Title !  \n",
       "2  It stars Shia LaBeouf as Sam Witwicky .  \n",
       "3               It was published in 2003 .  \n",
       "4            However , it is radioactive .  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset appears to be quite noisy, containing some nonsense examples such as the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pony Canyon Popfrenzy Popsicle Records Poptones Pork Recordings Portrait Records Posh Boy Records Positiva Records Positive Tone Positron !\n",
      "Pony Canyon Popfrenzy Poptones Pork Recordings Portrait Records Posh Boy Records Positiva Records Positive Tone Positron !\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print(train.loc[i, 'source'])\n",
    "print(train.loc[i, 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also contains some low-quality simplifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1199 , during the period of its highest splendour , the city made itself independent from the bishops of Volterra .\n",
      "Volterra .\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "print(train.loc[i, 'source'])\n",
    "print(train.loc[i, 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set, on the other hand, comes from the [Turk corpus](https://github.com/cocoxu/simplification/). It was produced by humans on Amazon's [Mechanical Turk](https://www.mturk.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genetic engineering has expanded the genes ava...</td>\n",
       "      <td>New plants were created with genetic engineeri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The name Jadgal applies specifically to groups...</td>\n",
       "      <td>The Jadgal people speak the Jadgali language .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wikipedia is free content that anyone can edit...</td>\n",
       "      <td>Wikipedia is free content that anyone may chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The print collections are further supported by...</td>\n",
       "      <td>The print collections include large collection...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Restoration of independence In 1991 , the Sovi...</td>\n",
       "      <td>Armenia received its independence from the Sov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  Genetic engineering has expanded the genes ava...   \n",
       "1  The name Jadgal applies specifically to groups...   \n",
       "2  Wikipedia is free content that anyone can edit...   \n",
       "3  The print collections are further supported by...   \n",
       "4  Restoration of independence In 1991 , the Sovi...   \n",
       "\n",
       "                                              target  \n",
       "0  New plants were created with genetic engineeri...  \n",
       "1     The Jadgal people speak the Jadgali language .  \n",
       "2  Wikipedia is free content that anyone may chan...  \n",
       "3  The print collections include large collection...  \n",
       "4  Armenia received its independence from the Sov...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
